{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d6dc316f",
      "metadata": {
        "id": "d6dc316f"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Spandan-Madan/generalization_to_OOD_category_viewpoint_cominations/blob/main/demos/separate_vs_shared.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4e6bdfe",
      "metadata": {
        "id": "b4e6bdfe"
      },
      "source": [
        "# Overview\n",
        "\n",
        "\n",
        "This demo shows the impact of parameter sharing on out-of-distribution generalization. Specifically, we show how the `SHARED` and `SEPARATE` architectures compare.\n",
        "\n",
        "As shown below, the `SHARED` architecture enforces parameter sharing between the two tasks (category prediction and viewpoint prediction), and the same weights are learned for all convolutional layers for both tasks. These are followed by two task specific fully connected layers for prediction. On the other hand, the `SEPARATE` architecture does not enforce any parameter sharing - all layers are learned separately for the two tasks.\n",
        "\n",
        "As described in the paper, our results show that while `SEPARATE` architecture can perform marginally worse on in-distribution combinations, they perform significantly better on out-of-distribution combinations."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42dc73b4",
      "metadata": {
        "id": "42dc73b4"
      },
      "source": [
        "`SHARED`             |  `SEPARATE`\n",
        ":-------------------------:|:-------------------------:\n",
        "![](https://github.com/Spandan-Madan/generalization_to_OOD_category_viewpoint_cominations/blob/main/docs/images/Shared.png?raw=1)  |  ![](https://github.com/Spandan-Madan/generalization_to_OOD_category_viewpoint_cominations/blob/main/docs/images/Separate.png?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "metropolitan-programmer",
      "metadata": {
        "id": "metropolitan-programmer"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "def create_folder(path):\n",
        "    if not os.path.isdir(path):\n",
        "        os.mkdir(path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38918b8c",
      "metadata": {
        "id": "38918b8c"
      },
      "source": [
        "If running on google colab, the below code does the following:\n",
        "- clone repo\n",
        "- set up necessary folders\n",
        "- download MNIST Rotation Dataset at appropriate place\n",
        "- unzip MNIST Rotation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd4b4a66",
      "metadata": {
        "id": "dd4b4a66"
      },
      "source": [
        "#### If you're not running on colab, please follow download instructions to get the mnist_rotaiton dataset using:\n",
        "\n",
        "```\n",
        "cd utils\n",
        "bash download_mnist_rotation.sh\n",
        "```\n",
        "\n",
        "#### If not using google colab, please proceed below only after downloading the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "korean-explorer",
      "metadata": {
        "id": "korean-explorer",
        "outputId": "34aec49c-2a39-4c72-8292-fb51a10f3f65",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning code base to colab....\n",
            "Cloning into 'generalization_to_OOD_category_viewpoint_cominations'...\n",
            "remote: Enumerating objects: 944, done.\u001b[K\n",
            "remote: Counting objects: 100% (944/944), done.\u001b[K\n",
            "remote: Compressing objects: 100% (583/583), done.\u001b[K\n",
            "remote: Total 944 (delta 497), reused 786 (delta 348), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (944/944), 232.54 MiB | 43.84 MiB/s, done.\n",
            "Resolving deltas: 100% (497/497), done.\n",
            "Checking out files: 100% (126/126), done.\n",
            "--2022-04-01 19:41:22--  https://www.dropbox.com/s/wdws3b3fjo190sk/self_generated.tar.gz?dl=0\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.5.18, 2620:100:601d:18::a27d:512\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.5.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/wdws3b3fjo190sk/self_generated.tar.gz [following]\n",
            "--2022-04-01 19:41:22--  https://www.dropbox.com/s/raw/wdws3b3fjo190sk/self_generated.tar.gz\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc5b810cfa58cb321880487c1695.dl.dropboxusercontent.com/cd/0/inline/BinzbcjEpB5KIfjQkjj0gmSJjrftbcYF-KWuBOCgsklYegIkHdpY3jcs9AaSRfedCkXI4B-nEPnalMkSQ-eF4K4tRsRz6NK9Q4fNWT1XowsKeZJdHEk6BNHJQFw1gclYbhAM7JYBYOoTDzlKDPs3R7w7PeLpxcL2YVhGQ8XBrcf9gA/file# [following]\n",
            "--2022-04-01 19:41:22--  https://uc5b810cfa58cb321880487c1695.dl.dropboxusercontent.com/cd/0/inline/BinzbcjEpB5KIfjQkjj0gmSJjrftbcYF-KWuBOCgsklYegIkHdpY3jcs9AaSRfedCkXI4B-nEPnalMkSQ-eF4K4tRsRz6NK9Q4fNWT1XowsKeZJdHEk6BNHJQFw1gclYbhAM7JYBYOoTDzlKDPs3R7w7PeLpxcL2YVhGQ8XBrcf9gA/file\n",
            "Resolving uc5b810cfa58cb321880487c1695.dl.dropboxusercontent.com (uc5b810cfa58cb321880487c1695.dl.dropboxusercontent.com)... 162.125.2.15, 2620:100:601d:15::a27d:50f\n",
            "Connecting to uc5b810cfa58cb321880487c1695.dl.dropboxusercontent.com (uc5b810cfa58cb321880487c1695.dl.dropboxusercontent.com)|162.125.2.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /cd/0/inline2/Bilu1RwijzO7GeT3SO2vDo7sVNOEf9xNIaUegL0_RSV41yLEKKSIUYA4dfsV8k9U_lpccHV0MPLeDUKh5_HBxhnkzk4iC8gj2NLXR5tQcXndmULXzIjn8EJqMjr9rZmHeNGoFGflIU0dFp2nwLjs-x79DGw04zLW4m8ZJd9vIuwBoqzbFb7bsCvI1T_kRjr_AGHN4t3r9Gs1iB9CXQSgi0_FiWAYE9C0JhHWCE2Je4U0u01plNmcEr9_Dhu8G5kbcyGssjwW2ogzMg5Ts-XWNlYIiuXJvDdXgann_yDtIWStWNd9_8u8Kqt9MsgOxhVlElF6Wbz8EI8wHkvcB1RTHNaNqGMqpGAEMVNJSldp0oVOKqD_pWyjrfkRLhZleGBGRbsiDtjg0FaMtuTNwJGtEyJTbarg8NMzNzeigO3qCi-pvw/file [following]\n",
            "--2022-04-01 19:41:23--  https://uc5b810cfa58cb321880487c1695.dl.dropboxusercontent.com/cd/0/inline2/Bilu1RwijzO7GeT3SO2vDo7sVNOEf9xNIaUegL0_RSV41yLEKKSIUYA4dfsV8k9U_lpccHV0MPLeDUKh5_HBxhnkzk4iC8gj2NLXR5tQcXndmULXzIjn8EJqMjr9rZmHeNGoFGflIU0dFp2nwLjs-x79DGw04zLW4m8ZJd9vIuwBoqzbFb7bsCvI1T_kRjr_AGHN4t3r9Gs1iB9CXQSgi0_FiWAYE9C0JhHWCE2Je4U0u01plNmcEr9_Dhu8G5kbcyGssjwW2ogzMg5Ts-XWNlYIiuXJvDdXgann_yDtIWStWNd9_8u8Kqt9MsgOxhVlElF6Wbz8EI8wHkvcB1RTHNaNqGMqpGAEMVNJSldp0oVOKqD_pWyjrfkRLhZleGBGRbsiDtjg0FaMtuTNwJGtEyJTbarg8NMzNzeigO3qCi-pvw/file\n",
            "Reusing existing connection to uc5b810cfa58cb321880487c1695.dl.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 140119693 (134M) [application/octet-stream]\n",
            "Saving to: ‘self_generated.tar.gz’\n",
            "\n",
            "self_generated.tar. 100%[===================>] 133.63M  43.4MB/s    in 3.2s    \n",
            "\n",
            "2022-04-01 19:41:27 (41.3 MB/s) - ‘self_generated.tar.gz’ saved [140119693/140119693]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "if 'google.colab' in str(get_ipython()):\n",
        "    print('Cloning code base to colab....')\n",
        "    !git clone https://github.com/Spandan-Madan/generalization_to_OOD_category_viewpoint_cominations.git\n",
        "    !cd generalization_to_OOD_category_viewpoint_cominations/utils && bash download_mnist_rotation.sh\n",
        "    CODE_ROOT = \"generalization_to_OOD_category_viewpoint_cominations/\"\n",
        "else:\n",
        "    CODE_ROOT = '..'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "introductory-mozambique",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "introductory-mozambique",
        "outputId": "e0471d80-f944-4991-d293-b90fb8dd1bcb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "generalization_to_OOD_category_viewpoint_cominations//res/loader/loader.py\n",
            "generalization_to_OOD_category_viewpoint_cominations//res/loader\n"
          ]
        }
      ],
      "source": [
        "from __future__ import print_function, division\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import copy\n",
        "import os\n",
        "from PIL import ImageFile\n",
        "import random\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "import argparse\n",
        "import pickle\n",
        "import sys\n",
        "sys.path.append('%s/res/'%CODE_ROOT)\n",
        "from models.models import get_model\n",
        "from loader.loader import get_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "protecting-picture",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "protecting-picture",
        "outputId": "40c0a092-33f0-4682-f422-263e164b24a7"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from tqdm.notebook import tqdm\n",
        "from IPython.display import clear_output\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set_context(\"poster\")\n",
        "sns.set_palette(\"Set1\", 8, .75)\n",
        "sns.despine()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "saved-craps",
      "metadata": {
        "id": "saved-craps"
      },
      "outputs": [],
      "source": [
        "DATASET_NAME = 'mnist_rotation_six_by_nine'\n",
        "NUM_EPOCHS = 10\n",
        "BATCH_SIZE = 200\n",
        "ARCH = 'LATE_BRANCHING_COMBINED'\n",
        "\n",
        "image_transform=transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.1307,), (0.3081,))\n",
        "    ])\n",
        "\n",
        "\n",
        "GPU = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "musical-thickness",
      "metadata": {
        "id": "musical-thickness",
        "tags": []
      },
      "outputs": [],
      "source": [
        "NUM_CLASSES = (10,10,10,10)\n",
        "loader_new = get_loader('multi_attribute_loader_file_list_mnist_rotation')\n",
        "\n",
        "file_list_root = '%s/dataset_lists/mnist_rotation_lists/'%CODE_ROOT\n",
        "att_path = '%s/dataset_lists/combined_attributes.p'%CODE_ROOT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "unsigned-tuner",
      "metadata": {
        "id": "unsigned-tuner"
      },
      "outputs": [],
      "source": [
        "shuffles = {'train':True,'val':True,'test':False}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "checked-assignment",
      "metadata": {
        "id": "checked-assignment"
      },
      "outputs": [],
      "source": [
        "data_dir = '%s/data/'%CODE_ROOT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "varied-stage",
      "metadata": {
        "id": "varied-stage"
      },
      "outputs": [],
      "source": [
        "file_lists = {}\n",
        "dsets = {}\n",
        "dset_loaders = {}\n",
        "dset_sizes = {}\n",
        "for phase in ['train','val','test']:\n",
        "    file_lists[phase] = \"%s/%s_list_%s.txt\"%(file_list_root,phase,DATASET_NAME)\n",
        "    dsets[phase] = loader_new(file_lists[phase],att_path, image_transform, data_dir)\n",
        "    dset_loaders[phase] = torch.utils.data.DataLoader(dsets[phase], batch_size=BATCH_SIZE, shuffle = shuffles[phase], num_workers=2,drop_last=True)\n",
        "    dset_sizes[phase] = len(dsets[phase])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "racial-praise",
      "metadata": {
        "id": "racial-praise"
      },
      "outputs": [],
      "source": [
        "multi_losses = [nn.CrossEntropyLoss(),nn.CrossEntropyLoss(),nn.CrossEntropyLoss(),nn.CrossEntropyLoss()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "incoming-press",
      "metadata": {
        "id": "incoming-press"
      },
      "outputs": [],
      "source": [
        "def weight_scheduler(epoch_num, task):\n",
        "    if task == 'shared':\n",
        "        return [0.0,1.0,0.0,1.0]\n",
        "    elif task == 'viewpoint':\n",
        "        return [0.0,1.0,0.0,0.0]\n",
        "    elif task == 'category':\n",
        "        return [0.0,0.0,0.0,1.0]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import ViTFeatureExtractor\n",
        "\n",
        "feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224')\n",
        "# encoding = feature_extractor(images=im, return_tensors=\"pt\")\n",
        "# encoding.keys()"
      ],
      "metadata": {
        "id": "GzOFqT11rZ28"
      },
      "id": "GzOFqT11rZ28",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "skilled-union",
      "metadata": {
        "id": "skilled-union"
      },
      "outputs": [],
      "source": [
        "def train_epoch(model, task, optimizer):\n",
        "    model.train()\n",
        "    torch.set_grad_enabled(True)\n",
        "    phase = 'train'\n",
        "    \n",
        "    weights = weight_scheduler(epoch, task)\n",
        "    iters = 0\n",
        "    phase_epoch_corrects = [0,0,0,0]\n",
        "    phase_epoch_loss = 0\n",
        "    \n",
        "    for data in tqdm(dset_loaders[phase]):\n",
        "        inputs, labels_all, paths = data\n",
        "        print(f\"inputs shape: {inputs.shape}\")\n",
        "        print(\"before\")\n",
        "        print(f\"inputs type:  {type(inputs)}\")\n",
        "        encoding = feature_extractor(images=inputs, return_tensors=\"pt\")\n",
        "        print(\"after\")\n",
        "        print(f\"encoding pixel shape: {encoding['pixel_values'].shape}\")\n",
        "        inputs = Variable(inputs.float().cuda())\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        model_outs = model(inputs)\n",
        "        calculated_loss = 0\n",
        "        batch_corrects = [0,0,0,0]\n",
        "        \n",
        "        for i in range(4):\n",
        "            labels = labels_all[:,i]\n",
        "            if GPU:\n",
        "                labels = Variable(labels.long().cuda())\n",
        "            loss = multi_losses[i]\n",
        "            outputs = model_outs[i]\n",
        "            calculated_loss += weights[i] * loss(outputs,labels)\n",
        "            _, preds = torch.max(outputs.data, 1)\n",
        "            batch_corrects[i] = torch.sum(preds == labels.data)\n",
        "            phase_epoch_corrects[i] += batch_corrects[i]\n",
        "\n",
        "        \n",
        "        phase_epoch_loss += calculated_loss\n",
        "        calculated_loss.backward()\n",
        "        optimizer.step()\n",
        "        iters += 1\n",
        "    epoch_loss = phase_epoch_loss/dset_sizes[phase]\n",
        "    # print('Train loss:%s'%epoch_loss)\n",
        "    epoch_accs = [float(i)/dset_sizes[phase] for i in phase_epoch_corrects]\n",
        "\n",
        "    if task == 'shared':\n",
        "        epoch_gm = np.sqrt(epoch_accs[1] * epoch_accs[3])\n",
        "    elif task == 'viewpoint':\n",
        "        epoch_gm = epoch_accs[1]\n",
        "    elif task == 'category':\n",
        "        epoch_gm = epoch_accs[3]\n",
        "    \n",
        "    return model, epoch_loss, epoch_gm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "integrated-binary",
      "metadata": {
        "id": "integrated-binary"
      },
      "outputs": [],
      "source": [
        "def test_epoch(model, best_model, best_test_loss, best_test_gm, task):\n",
        "    model.eval()\n",
        "    torch.set_grad_enabled(False)\n",
        "    phase = 'val'\n",
        "    weights = weight_scheduler(epoch, task)\n",
        "    iters = 0\n",
        "    phase_epoch_corrects = [0,0,0,0]\n",
        "    phase_epoch_loss = 0\n",
        "    \n",
        "    for data in tqdm(dset_loaders[phase]):\n",
        "        inputs, labels_all, paths = data\n",
        "        inputs = Variable(inputs.float().cuda())\n",
        "        model_outs = model(inputs)\n",
        "        calculated_loss = 0\n",
        "        batch_corrects = [0,0,0,0]\n",
        "        \n",
        "        for i in range(4):\n",
        "            labels = labels_all[:,i]\n",
        "            if GPU:\n",
        "                labels = Variable(labels.long().cuda())\n",
        "            loss = multi_losses[i]\n",
        "            outputs = model_outs[i]\n",
        "            calculated_loss += weights[i] * loss(outputs,labels)\n",
        "            _, preds = torch.max(outputs.data, 1)\n",
        "            batch_corrects[i] = torch.sum(preds == labels.data)\n",
        "            phase_epoch_corrects[i] += batch_corrects[i]\n",
        "\n",
        "\n",
        "        phase_epoch_loss += calculated_loss\n",
        "        iters += 1\n",
        "    epoch_loss = phase_epoch_loss/dset_sizes[phase]\n",
        "    # print('Test loss:%s'%epoch_loss)\n",
        "    epoch_accs = [float(i)/dset_sizes[phase] for i in phase_epoch_corrects]\n",
        "    \n",
        "    if task == 'shared':\n",
        "        epoch_gm = np.sqrt(epoch_accs[1] * epoch_accs[3])\n",
        "    elif task == 'viewpoint':\n",
        "        epoch_gm = epoch_accs[1]\n",
        "    elif task == 'category':\n",
        "        epoch_gm = epoch_accs[3]\n",
        "    \n",
        "    if epoch_loss < best_test_loss:\n",
        "        best_model = model\n",
        "        best_test_loss = epoch_loss\n",
        "        best_test_gm = epoch_gm\n",
        "    \n",
        "    return best_model, epoch_loss, epoch_gm, best_test_loss, best_test_gm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "selected-tulsa",
      "metadata": {
        "id": "selected-tulsa"
      },
      "outputs": [],
      "source": [
        "def unseen_test_epoch(model, task):\n",
        "    model.eval()\n",
        "    torch.set_grad_enabled(False)\n",
        "    phase = 'test'\n",
        "\n",
        "    weights = weight_scheduler(epoch, task)\n",
        "    iters = 0\n",
        "    phase_epoch_corrects = [0,0,0,0]\n",
        "    phase_epoch_loss = 0\n",
        "    \n",
        "    for data in tqdm(dset_loaders[phase]):\n",
        "        inputs, labels_all, paths = data\n",
        "        inputs = Variable(inputs.float().cuda())\n",
        "        model_outs = model(inputs)\n",
        "        calculated_loss = 0\n",
        "        batch_corrects = [0,0,0,0]\n",
        "        \n",
        "        for i in range(4):\n",
        "            labels = labels_all[:,i]\n",
        "            if GPU:\n",
        "                labels = Variable(labels.long().cuda())\n",
        "            loss = multi_losses[i]\n",
        "            outputs = model_outs[i]\n",
        "            calculated_loss += weights[i] * loss(outputs,labels)\n",
        "            _, preds = torch.max(outputs.data, 1)\n",
        "            batch_corrects[i] = torch.sum(preds == labels.data)\n",
        "            phase_epoch_corrects[i] += batch_corrects[i]\n",
        "\n",
        "\n",
        "        phase_epoch_loss += calculated_loss\n",
        "        iters += 1\n",
        "    epoch_loss = phase_epoch_loss/dset_sizes[phase]\n",
        "    epoch_accs = [float(i)/dset_sizes[phase] for i in phase_epoch_corrects]\n",
        "    \n",
        "    if task == 'shared':\n",
        "        epoch_gm = np.sqrt(epoch_accs[1] * epoch_accs[3])\n",
        "    elif task == 'viewpoint':\n",
        "        epoch_gm = epoch_accs[1]\n",
        "    elif task == 'category':\n",
        "        epoch_gm = epoch_accs[3]\n",
        "    \n",
        "    return epoch_loss, epoch_gm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#pip install -q git+https://github.com/huggingface/transformers.git"
      ],
      "metadata": {
        "id": "LXq3T7uxXYGo"
      },
      "id": "LXq3T7uxXYGo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from transformers import ViTForImageClassification\n",
        "# import torch\n",
        "\n",
        "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')\n",
        "# model.eval()\n",
        "# model.to(device)\n",
        "# #print(model)"
      ],
      "metadata": {
        "id": "an-240vESkl3"
      },
      "id": "an-240vESkl3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from transformers import ViTFeatureExtractor\n",
        "\n",
        "# feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224')\n",
        "# encoding = feature_extractor(images=im, return_tensors=\"pt\")\n",
        "# encoding.keys()"
      ],
      "metadata": {
        "id": "C7xsTITNXzKj"
      },
      "id": "C7xsTITNXzKj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import ViTForImageClassification\n",
        "import torch\n",
        "\n",
        "\n",
        "class MyModel(nn.Module):\n",
        "    def __init__(self, num_classes1, num_classes2):\n",
        "        super(MyModel, self).__init__()\n",
        "        self.num_classes1 = num_classes1\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')\n",
        "        #self.model_resnet = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True)\n",
        "        #num_ftrs = self.model_resnet.fc.in_features\n",
        "        #self.model_resnet.fc = nn.Identity() #not sure this is necessary\n",
        "        self.model.classifier  = nn.Linear(768, num_classes1 + num_classes2 )\n",
        "        # self.fc1 = nn.Linear(1000, num_classes1) #1000 outputs default from resnet\n",
        "        # self.fc2 = nn.Linear(1000, num_classes2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.model(x)\n",
        "        #print(x.shape)\n",
        "        out1 = x[:,0:self.num_classes1]\n",
        "        out2 = x[:, self.num_classes1:]\n",
        "        # out1 = self.fc1(x)\n",
        "        # out2 = self.fc2(x)\n",
        "        return out1, out1, out2, out2\n",
        "    \n",
        "model = MyModel(10,10)\n",
        "print(model)"
      ],
      "metadata": {
        "id": "sWBvgvLGR7Lp",
        "outputId": "ee87237e-5f51-4e24-9b27-2da40f4b7016",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "sWBvgvLGR7Lp",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MyModel(\n",
            "  (model): ViTForImageClassification(\n",
            "    (vit): ViTModel(\n",
            "      (embeddings): ViTEmbeddings(\n",
            "        (patch_embeddings): PatchEmbeddings(\n",
            "          (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
            "        )\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (encoder): ViTEncoder(\n",
            "        (layer): ModuleList(\n",
            "          (0): ViTLayer(\n",
            "            (attention): ViTAttention(\n",
            "              (attention): ViTSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.0, inplace=False)\n",
            "              )\n",
            "              (output): ViTSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.0, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): ViTIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (intermediate_act_fn): GELUActivation()\n",
            "            )\n",
            "            (output): ViTOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "            (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          )\n",
            "          (1): ViTLayer(\n",
            "            (attention): ViTAttention(\n",
            "              (attention): ViTSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.0, inplace=False)\n",
            "              )\n",
            "              (output): ViTSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.0, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): ViTIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (intermediate_act_fn): GELUActivation()\n",
            "            )\n",
            "            (output): ViTOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "            (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          )\n",
            "          (2): ViTLayer(\n",
            "            (attention): ViTAttention(\n",
            "              (attention): ViTSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.0, inplace=False)\n",
            "              )\n",
            "              (output): ViTSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.0, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): ViTIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (intermediate_act_fn): GELUActivation()\n",
            "            )\n",
            "            (output): ViTOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "            (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          )\n",
            "          (3): ViTLayer(\n",
            "            (attention): ViTAttention(\n",
            "              (attention): ViTSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.0, inplace=False)\n",
            "              )\n",
            "              (output): ViTSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.0, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): ViTIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (intermediate_act_fn): GELUActivation()\n",
            "            )\n",
            "            (output): ViTOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "            (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          )\n",
            "          (4): ViTLayer(\n",
            "            (attention): ViTAttention(\n",
            "              (attention): ViTSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.0, inplace=False)\n",
            "              )\n",
            "              (output): ViTSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.0, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): ViTIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (intermediate_act_fn): GELUActivation()\n",
            "            )\n",
            "            (output): ViTOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "            (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          )\n",
            "          (5): ViTLayer(\n",
            "            (attention): ViTAttention(\n",
            "              (attention): ViTSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.0, inplace=False)\n",
            "              )\n",
            "              (output): ViTSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.0, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): ViTIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (intermediate_act_fn): GELUActivation()\n",
            "            )\n",
            "            (output): ViTOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "            (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          )\n",
            "          (6): ViTLayer(\n",
            "            (attention): ViTAttention(\n",
            "              (attention): ViTSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.0, inplace=False)\n",
            "              )\n",
            "              (output): ViTSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.0, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): ViTIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (intermediate_act_fn): GELUActivation()\n",
            "            )\n",
            "            (output): ViTOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "            (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          )\n",
            "          (7): ViTLayer(\n",
            "            (attention): ViTAttention(\n",
            "              (attention): ViTSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.0, inplace=False)\n",
            "              )\n",
            "              (output): ViTSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.0, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): ViTIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (intermediate_act_fn): GELUActivation()\n",
            "            )\n",
            "            (output): ViTOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "            (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          )\n",
            "          (8): ViTLayer(\n",
            "            (attention): ViTAttention(\n",
            "              (attention): ViTSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.0, inplace=False)\n",
            "              )\n",
            "              (output): ViTSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.0, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): ViTIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (intermediate_act_fn): GELUActivation()\n",
            "            )\n",
            "            (output): ViTOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "            (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          )\n",
            "          (9): ViTLayer(\n",
            "            (attention): ViTAttention(\n",
            "              (attention): ViTSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.0, inplace=False)\n",
            "              )\n",
            "              (output): ViTSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.0, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): ViTIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (intermediate_act_fn): GELUActivation()\n",
            "            )\n",
            "            (output): ViTOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "            (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          )\n",
            "          (10): ViTLayer(\n",
            "            (attention): ViTAttention(\n",
            "              (attention): ViTSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.0, inplace=False)\n",
            "              )\n",
            "              (output): ViTSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.0, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): ViTIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (intermediate_act_fn): GELUActivation()\n",
            "            )\n",
            "            (output): ViTOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "            (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          )\n",
            "          (11): ViTLayer(\n",
            "            (attention): ViTAttention(\n",
            "              (attention): ViTSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.0, inplace=False)\n",
            "              )\n",
            "              (output): ViTSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.0, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): ViTIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (intermediate_act_fn): GELUActivation()\n",
            "            )\n",
            "            (output): ViTOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "            (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "    )\n",
            "    (classifier): Linear(in_features=768, out_features=20, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "automotive-glossary",
      "metadata": {
        "id": "automotive-glossary"
      },
      "source": [
        "Separate vs Shared Networks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "described-triumph",
      "metadata": {
        "id": "described-triumph"
      },
      "outputs": [],
      "source": [
        "models = {} \n",
        "\n",
        "from torchsummary import summary\n",
        "\n",
        "#models['shared']= get_model(ARCH,NUM_CLASSES)\n",
        "#models['viewpoint']= get_model(ARCH,NUM_CLASSES)\n",
        "#models['category']= get_model(ARCH,NUM_CLASSES)\n",
        "\n",
        "###############################################\n",
        "models['shared']= model\n",
        "models['viewpoint']= model\n",
        "models['category']= model\n",
        "###############################################\n",
        "\n",
        "models['shared'].cuda();\n",
        "#summary(models['shared'], (3, 28, 28)) #print a model summary\n",
        "models['viewpoint'].cuda();\n",
        "models['category'].cuda();\n",
        "\n",
        "best_models = {}\n",
        "best_models['shared'] = models['shared']\n",
        "best_models['viewpoint'] = models['viewpoint']\n",
        "best_models['category'] = models['category']\n",
        "\n",
        "best_test_loss = 100\n",
        "best_test_gm = 0\n",
        "\n",
        "all_train_gms = {}\n",
        "all_train_gms['shared'] = [0]\n",
        "all_train_gms['separate'] = [0]\n",
        "\n",
        "all_test_gms = {}\n",
        "all_test_gms['shared'] = [0]\n",
        "all_test_gms['separate'] = [0]\n",
        "\n",
        "all_unseen_test_gms = {}\n",
        "all_unseen_test_gms['shared'] = [0]\n",
        "all_unseen_test_gms['separate'] = [0]\n",
        "\n",
        "optimizers = {}\n",
        "optimizers['shared'] = optim.Adam(models['shared'].parameters(), lr=0.001)\n",
        "optimizers['viewpoint'] = optim.Adam(models['viewpoint'].parameters(), lr=0.001)\n",
        "optimizers['category'] = optim.Adam(models['category'].parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "satisfactory-joshua",
      "metadata": {
        "id": "satisfactory-joshua"
      },
      "outputs": [],
      "source": [
        "plt.rc('xtick', labelsize=14) \n",
        "plt.rc('ytick', labelsize=14) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "commercial-difference",
      "metadata": {
        "id": "commercial-difference",
        "outputId": "5ae52c6a-b79a-4d02-a442-b0975d71fe8e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 552,
          "referenced_widgets": [
            "72ae966324ba40198d6233e174b4fb19",
            "2b88dce1f1774eb6ac20e5d8bdfcf080",
            "92a8d7e1a61d489b8c51657439b4458a",
            "b7d71792986e48be9146de575de142e5",
            "8fd96cddf8d84039865e6afd106153b3",
            "620d6985984b423482f8d792f46309de",
            "61f2647bd5434b0887a14a2a411303dc",
            "84d4d8b094bb43ba8fb253d655733427",
            "b0bc0b01d637497589dd041fc158e367",
            "07206a1dfb7f4eea814ab51dfb838953",
            "036a2b0934924a96ab2760e217779807"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Task: viewpoint\n",
            "---------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/250 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "72ae966324ba40198d6233e174b4fb19"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs shape: torch.Size([200, 3, 28, 28])\n",
            "before\n",
            "inputs type:  <class 'torch.Tensor'>\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mfromarray\u001b[0;34m(obj, mode)\u001b[0m\n\u001b[1;32m   2713\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2714\u001b[0;31m             \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrawmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_fromarray_typemap\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtypekey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2715\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: ((1, 1, 28, 28), '|u1')",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-91-6fb64dcc8d73>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch: %s, Task: %s'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mTASK\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'---------'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTASK\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_gm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTASK\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTASK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTASK\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mbest_models\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTASK\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_gm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_test_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_test_gm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTASK\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_models\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTASK\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_test_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_test_gm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTASK\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0munseen_test_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munseen_test_gm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munseen_test_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTASK\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTASK\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-82-714bef3a8e9a>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, task, optimizer)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"before\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"inputs type:  {type(inputs)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_extractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"after\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"encoding pixel shape: {encoding['pixel_values'].shape}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/vit/feature_extraction_vit.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, images, return_tensors, **kwargs)\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;31m# transformations (resizing + normalization)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_resize\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresample\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_normalize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_std\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/vit/feature_extraction_vit.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;31m# transformations (resizing + normalization)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_resize\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresample\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_normalize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_std\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/image_utils.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(self, image, size, resample, default_to_square, max_size)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPIL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m             \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_pil_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/image_utils.py\u001b[0m in \u001b[0;36mto_pil_image\u001b[0;34m(self, image, rescale)\u001b[0m\n\u001b[1;32m    114\u001b[0m                 \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mPIL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mfromarray\u001b[0;34m(obj, mode)\u001b[0m\n\u001b[1;32m   2714\u001b[0m             \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrawmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_fromarray_typemap\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtypekey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2715\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2716\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot handle this data type: %s, %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtypekey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2717\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2718\u001b[0m         \u001b[0mrawmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Cannot handle this data type: (1, 1, 28, 28), |u1"
          ]
        }
      ],
      "source": [
        "for epoch in range(10):\n",
        "    train_gm_separate = 1\n",
        "    test_gm_separate = 1\n",
        "    unseen_test_gm_separate = 1\n",
        "\n",
        "    for TASK in ['viewpoint','category','shared']:\n",
        "        print('Epoch: %s, Task: %s'%(epoch,TASK))\n",
        "        print('---------')\n",
        "        models[TASK], train_loss, train_gm = train_epoch(models[TASK], TASK, optimizers[TASK])\n",
        "        best_models[TASK], test_loss, test_gm, best_test_loss, best_test_gm = test_epoch(models[TASK], best_models[TASK], best_test_loss, best_test_gm, TASK)\n",
        "        unseen_test_loss, unseen_test_gm = unseen_test_epoch(models[TASK], TASK)\n",
        "\n",
        "        if TASK != 'shared':\n",
        "            train_gm_separate = train_gm_separate * train_gm\n",
        "            test_gm_separate = test_gm_separate * test_gm\n",
        "            unseen_test_gm_separate = unseen_test_gm_separate * test_gm\n",
        "\n",
        "    all_train_gms['separate'].append(np.sqrt(train_gm_separate))\n",
        "    all_test_gms['separate'].append(np.sqrt(test_gm_separate))\n",
        "    all_unseen_test_gms['separate'].append(np.sqrt(unseen_test_gm_separate))\n",
        "    all_train_gms['shared'].append(train_gm)\n",
        "    all_test_gms['shared'].append(test_gm)\n",
        "    all_unseen_test_gms['shared'].append(np.sqrt(unseen_test_gm))\n",
        "    \n",
        "    clear_output()\n",
        "    print('Epoch: %s'%epoch)\n",
        "    print('---------')\n",
        "    \n",
        "    line_labels = [\"Separate\", \"Shared\"]\n",
        "\n",
        "    fig,ax = plt.subplots(1, 3, figsize=(20,6))\n",
        "    l1 = ax[0].plot(all_train_gms['separate'], color = 'blue', marker = 'o', markersize=5)[0]\n",
        "    l2 = ax[0].plot(all_train_gms['shared'], color = 'red', marker = 'o', markersize=5)[0]\n",
        "    ax[0].set_title('Train Accuracy', fontsize=12)\n",
        "    ax[0].set_ylim(0,1.05)\n",
        "\n",
        "    ax[1].plot(all_test_gms['separate'], color = 'blue', marker = 'o', markersize=5)\n",
        "    ax[1].plot(all_test_gms['shared'], color = 'red', marker = 'o', markersize=5)\n",
        "    ax[1].set_title('Test Accuracy on Seen \\n Category-Viewpoint Combinations', fontsize=12)\n",
        "    ax[1].set_ylim(0,1.05)\n",
        "\n",
        "    ax[2].plot(all_unseen_test_gms['separate'], color = 'blue', marker = 'o', markersize=5)\n",
        "    ax[2].plot(all_unseen_test_gms['shared'], color = 'red', marker = 'o', markersize=5)\n",
        "    ax[2].set_ylim(0,1.05)\n",
        "    ax[2].set_title('Test Accuracy on Unseen \\n Category-Viewpoint Combinations', fontsize=12)\n",
        "    fig.legend([l1, l2],     # The line objects\n",
        "            labels=line_labels,   # The labels for each line\n",
        "            loc=\"center right\",   # Position of legend\n",
        "            borderaxespad=0.2,    # Small spacing around legend box\n",
        "            prop={\"size\":20})\n",
        "    plt.subplots_adjust(right=0.85)\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "certified-blair",
      "metadata": {
        "id": "certified-blair"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "separate_vs_shared_ViT.ipynb",
      "provenance": [],
      "background_execution": "on"
    },
    "kernelspec": {
      "display_name": "generalization_to_ood",
      "language": "python",
      "name": "generalization_to_ood"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "72ae966324ba40198d6233e174b4fb19": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2b88dce1f1774eb6ac20e5d8bdfcf080",
              "IPY_MODEL_92a8d7e1a61d489b8c51657439b4458a",
              "IPY_MODEL_b7d71792986e48be9146de575de142e5"
            ],
            "layout": "IPY_MODEL_8fd96cddf8d84039865e6afd106153b3"
          }
        },
        "2b88dce1f1774eb6ac20e5d8bdfcf080": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_620d6985984b423482f8d792f46309de",
            "placeholder": "​",
            "style": "IPY_MODEL_61f2647bd5434b0887a14a2a411303dc",
            "value": "  0%"
          }
        },
        "92a8d7e1a61d489b8c51657439b4458a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_84d4d8b094bb43ba8fb253d655733427",
            "max": 250,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b0bc0b01d637497589dd041fc158e367",
            "value": 0
          }
        },
        "b7d71792986e48be9146de575de142e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_07206a1dfb7f4eea814ab51dfb838953",
            "placeholder": "​",
            "style": "IPY_MODEL_036a2b0934924a96ab2760e217779807",
            "value": " 0/250 [00:00&lt;?, ?it/s]"
          }
        },
        "8fd96cddf8d84039865e6afd106153b3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "620d6985984b423482f8d792f46309de": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "61f2647bd5434b0887a14a2a411303dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "84d4d8b094bb43ba8fb253d655733427": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b0bc0b01d637497589dd041fc158e367": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "07206a1dfb7f4eea814ab51dfb838953": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "036a2b0934924a96ab2760e217779807": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}